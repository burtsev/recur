INFO - 07/18/22 17:55:24 - 0:00:00 - ============ Initialized logger ============
INFO - 07/18/22 17:55:24 - 0:00:00 - accumulate_gradients: 1
                                     amp: -1
                                     attention_dropout: 0
                                     batch_load: False
                                     batch_size: 32
                                     batch_size_eval: None
                                     beam_early_stopping: True
                                     beam_eval: True
                                     beam_eval_train: 0
                                     beam_length_penalty: 1
                                     beam_size: 1
                                     clip_grad_norm: 1
                                     command: python train.py --cpu True --exp_id "6s1tkzdd5b"
                                     cpu: True
                                     curriculum_n_ops: False
                                     debug: False
                                     debug_slurm: False
                                     dec_emb_dim: 128
                                     dimension: 1
                                     dropout: 0
                                     dump_path: ./debug/6s1tkzdd5b
                                     enc_emb_dim: 128
                                     env_base_seed: -1
                                     env_name: recurrence
                                     epoch_size: 10000
                                     eval_data: 
                                     eval_from_exp: 
                                     eval_input_length_modulo: -1
                                     eval_noise: 0
                                     eval_only: False
                                     eval_size: 1000
                                     eval_verbose: 0
                                     eval_verbose_print: False
                                     exp_id: 6s1tkzdd5b
                                     exp_name: debug
                                     export_data: False
                                     extra_binary_operators: 
                                     extra_constants: None
                                     extra_unary_operators: 
                                     float_constants: None
                                     float_precision: 3
                                     float_sequences: False
                                     float_tolerance: 1e-10
                                     fp16: False
                                     global_rank: 0
                                     init_scale: 10
                                     int_base: 10000
                                     is_master: True
                                     is_slurm_job: False
                                     local_rank: 0
                                     mantissa_len: 1
                                     master_port: -1
                                     max_degree: 6
                                     max_epoch: 100000
                                     max_exponent: 100
                                     max_int: 10
                                     max_len: 30
                                     max_number: 1e+100
                                     max_ops: 10
                                     max_output_len: 64
                                     max_token_len: 0
                                     min_len: 5
                                     min_op_prob: 0.01
                                     more_tolerance: 0.0,1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1
                                     multi_gpu: False
                                     multi_node: False
                                     n_dec_heads: 8
                                     n_dec_hidden_layers: 1
                                     n_dec_layers: 2
                                     n_enc_heads: 8
                                     n_enc_hidden_layers: 1
                                     n_enc_layers: 2
                                     n_gpu_per_node: 1
                                     n_nodes: 1
                                     n_predictions: 10
                                     node_id: 0
                                     norm_attention: False
                                     num_workers: 10
                                     nvidia_apex: False
                                     operators_to_remove: 
                                     optimizer: adam_inverse_sqrt,lr=0.0002,warmup_updates=10000
                                     output_numeric: False
                                     print_freq: 10
                                     prob_const: 0.3333333333333333
                                     prob_n: 0.3333333333333333
                                     prob_rand: 0.0
                                     reload_checkpoint: 
                                     reload_data: 
                                     reload_model: 
                                     reload_size: -1
                                     required_operators: 
                                     save_periodic: 25
                                     share_inout_emb: True
                                     simplify: False
                                     sinusoidal_embeddings: False
                                     stopping_criterion: 
                                     tasks: recurrence
                                     test_env_seed: 1
                                     train_noise: 0
                                     use_sympy: False
                                     validation_metrics: 
                                     windows: False
                                     world_size: 1
INFO - 07/18/22 17:55:24 - 0:00:00 - The experiment will be stored in ./debug/6s1tkzdd5b
                                     
INFO - 07/18/22 17:55:24 - 0:00:00 - Running command: python train.py --cpu True

INFO - 07/18/22 17:55:24 - 0:00:00 - vocabulary: 10011 input words, 51 output_words
INFO - 07/18/22 17:55:24 - 0:00:00 - Training tasks: recurrence
DEBUG - 07/18/22 17:55:24 - 0:00:00 - TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(10011, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                      ): TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(10011, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                      )
DEBUG - 07/18/22 17:55:24 - 0:00:00 - TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(51, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (proj): Linear(in_features=128, out_features=51, bias=True)
                                      ): TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(51, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (proj): Linear(in_features=128, out_features=51, bias=True)
                                      )
INFO - 07/18/22 17:55:24 - 0:00:00 - Number of parameters (encoder): 2202496
INFO - 07/18/22 17:55:24 - 0:00:00 - Number of parameters (decoder): 1060275
INFO - 07/18/22 17:55:24 - 0:00:00 - Found 93 parameters in model.
INFO - 07/18/22 17:55:24 - 0:00:00 - Optimizer: <class 'src.optim.AdamInverseSqrtWithWarmup'>
INFO - 07/18/22 17:55:24 - 0:00:00 - Creating train iterator for recurrence ...
INFO - 07/18/22 17:55:39 - 0:00:15 - Creating valid1 iterator for recurrence ...
INFO - 07/18/22 17:55:41 - 0:00:17 - (48/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
