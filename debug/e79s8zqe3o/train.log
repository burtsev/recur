INFO - 07/18/22 18:03:34 - 0:00:00 - ============ Initialized logger ============
INFO - 07/18/22 18:03:34 - 0:00:00 - accumulate_gradients: 1
                                     amp: -1
                                     attention_dropout: 0
                                     batch_load: False
                                     batch_size: 32
                                     batch_size_eval: None
                                     beam_early_stopping: True
                                     beam_eval: True
                                     beam_eval_train: 0
                                     beam_length_penalty: 1
                                     beam_size: 1
                                     clip_grad_norm: 1
                                     command: python train.py --cpu True --exp_id "e79s8zqe3o"
                                     cpu: True
                                     curriculum_n_ops: False
                                     debug: False
                                     debug_slurm: False
                                     dec_emb_dim: 128
                                     dimension: 1
                                     dropout: 0
                                     dump_path: ./debug/e79s8zqe3o
                                     enc_emb_dim: 128
                                     env_base_seed: -1
                                     env_name: recurrence
                                     epoch_size: 10000
                                     eval_data: 
                                     eval_from_exp: 
                                     eval_input_length_modulo: -1
                                     eval_noise: 0
                                     eval_only: False
                                     eval_size: 1000
                                     eval_verbose: 0
                                     eval_verbose_print: False
                                     exp_id: e79s8zqe3o
                                     exp_name: debug
                                     export_data: False
                                     extra_binary_operators: 
                                     extra_constants: None
                                     extra_unary_operators: 
                                     float_constants: None
                                     float_precision: 3
                                     float_sequences: False
                                     float_tolerance: 1e-10
                                     fp16: False
                                     global_rank: 0
                                     init_scale: 10
                                     int_base: 10000
                                     is_master: True
                                     is_slurm_job: False
                                     local_rank: 0
                                     mantissa_len: 1
                                     master_port: -1
                                     max_degree: 6
                                     max_epoch: 100000
                                     max_exponent: 100
                                     max_int: 10
                                     max_len: 30
                                     max_number: 1e+100
                                     max_ops: 10
                                     max_output_len: 64
                                     max_token_len: 0
                                     min_len: 5
                                     min_op_prob: 0.01
                                     more_tolerance: 0.0,1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1
                                     multi_gpu: False
                                     multi_node: False
                                     n_dec_heads: 8
                                     n_dec_hidden_layers: 1
                                     n_dec_layers: 2
                                     n_enc_heads: 8
                                     n_enc_hidden_layers: 1
                                     n_enc_layers: 2
                                     n_gpu_per_node: 1
                                     n_nodes: 1
                                     n_predictions: 10
                                     node_id: 0
                                     norm_attention: False
                                     num_workers: 10
                                     nvidia_apex: False
                                     operators_to_remove: 
                                     optimizer: adam_inverse_sqrt,lr=0.0002,warmup_updates=10000
                                     output_numeric: False
                                     print_freq: 10
                                     prob_const: 0.3333333333333333
                                     prob_n: 0.3333333333333333
                                     prob_rand: 0.0
                                     reload_checkpoint: 
                                     reload_data: 
                                     reload_model: 
                                     reload_size: -1
                                     required_operators: 
                                     save_periodic: 25
                                     share_inout_emb: True
                                     simplify: False
                                     sinusoidal_embeddings: False
                                     stopping_criterion: 
                                     tasks: recurrence
                                     test_env_seed: 1
                                     train_noise: 0
                                     use_sympy: False
                                     validation_metrics: 
                                     world_size: 1
INFO - 07/18/22 18:03:34 - 0:00:00 - The experiment will be stored in ./debug/e79s8zqe3o
                                     
INFO - 07/18/22 18:03:34 - 0:00:00 - Running command: python train.py --cpu True

INFO - 07/18/22 18:03:34 - 0:00:00 - vocabulary: 10011 input words, 51 output_words
INFO - 07/18/22 18:03:34 - 0:00:00 - Training tasks: recurrence
DEBUG - 07/18/22 18:03:34 - 0:00:00 - TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(10011, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                      ): TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(10011, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                      )
DEBUG - 07/18/22 18:03:34 - 0:00:00 - TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(51, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (proj): Linear(in_features=128, out_features=51, bias=True)
                                      ): TransformerModel(
                                        (position_embeddings): Embedding(4096, 128)
                                        (embeddings): Embedding(51, 128, padding_idx=44)
                                        (layer_norm_emb): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (midlin): ModuleList()
                                            (lin1): Linear(in_features=128, out_features=512, bias=True)
                                            (lin2): Linear(in_features=512, out_features=128, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (k_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (v_lin): Linear(in_features=128, out_features=128, bias=True)
                                            (out_lin): Linear(in_features=128, out_features=128, bias=True)
                                          )
                                        )
                                        (proj): Linear(in_features=128, out_features=51, bias=True)
                                      )
INFO - 07/18/22 18:03:34 - 0:00:00 - Number of parameters (encoder): 2202496
INFO - 07/18/22 18:03:34 - 0:00:00 - Number of parameters (decoder): 1060275
INFO - 07/18/22 18:03:34 - 0:00:00 - Found 93 parameters in model.
INFO - 07/18/22 18:03:34 - 0:00:00 - Optimizer: <class 'src.optim.AdamInverseSqrtWithWarmup'>
INFO - 07/18/22 18:03:34 - 0:00:00 - Creating train iterator for recurrence ...
INFO - 07/18/22 18:03:48 - 0:00:14 - Creating valid1 iterator for recurrence ...
INFO - 07/18/22 18:03:50 - 0:00:16 - (48/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:50 - 0:00:16 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:50 - 0:00:16 - (96/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:50 - 0:00:17 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:51 - 0:00:17 - (144/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:51 - 0:00:17 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:51 - 0:00:17 - (192/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:51 - 0:00:17 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:51 - 0:00:17 - (240/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:51 - 0:00:17 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:51 - 0:00:17 - (288/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:51 - 0:00:18 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:52 - 0:00:18 - (336/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:52 - 0:00:18 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:52 - 0:00:18 - (384/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:52 - 0:00:18 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:52 - 0:00:19 - (432/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:52 - 0:00:19 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:53 - 0:00:19 - (480/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:53 - 0:00:19 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:53 - 0:00:19 - (528/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:53 - 0:00:19 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:53 - 0:00:19 - (576/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:53 - 0:00:19 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:54 - 0:00:20 - (624/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:54 - 0:00:20 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:54 - 0:00:20 - (672/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:54 - 0:00:20 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:54 - 0:00:20 - (720/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:54 - 0:00:20 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:54 - 0:00:20 - (768/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:54 - 0:00:20 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:55 - 0:00:21 - (816/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:55 - 0:00:21 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:55 - 0:00:21 - (864/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:55 - 0:00:21 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:55 - 0:00:21 - (912/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:55 - 0:00:21 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:56 - 0:00:22 - (960/1000) Found 0/48 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:56 - 0:00:22 -     Found 0/48 solutions in beam hypotheses.
INFO - 07/18/22 18:03:56 - 0:00:22 - (1000/1000) Found 0/40 valid top-1 predictions. Generating solutions ...
INFO - 07/18/22 18:03:56 - 0:00:22 -     Found 0/40 solutions in beam hypotheses.
INFO - 07/18/22 18:04:01 - 0:00:27 - 0/1000 (0.0%) equations were evaluated correctly.
INFO - 07/18/22 18:04:01 - 0:00:27 - __log__:{"epoch": 0, "valid1_recurrence_xe_loss": 84.20518622970582, "valid1_recurrence_perfect": 0.0, "valid1_recurrence_correct": 0.0, "valid1_recurrence_beam_acc": 0.0, "valid1_recurrence_additional_1": 0.0, "valid1_recurrence_additional_2": 0.0, "valid1_recurrence_additional_3": 0.0, "valid1_recurrence_additional_4": 0.0, "valid1_recurrence_additional_5": 0.0, "valid1_recurrence_additional_6": 0.0, "valid1_recurrence_additional_7": 0.0, "valid1_recurrence_additional_8": 0.0, "valid1_recurrence_additional_9": 0.0, "valid1_recurrence_additional_10": 0.0, "valid1_recurrence_additional_11": 0.0, "valid1_recurrence_beam_acc_n_predictions_1": 0.0, "valid1_recurrence_beam_acc_n_predictions_2": 0.0, "valid1_recurrence_beam_acc_n_predictions_3": 0.0, "valid1_recurrence_beam_acc_n_predictions_4": 0.0, "valid1_recurrence_beam_acc_n_predictions_5": 0.0, "valid1_recurrence_beam_acc_n_predictions_6": 0.0, "valid1_recurrence_beam_acc_n_predictions_7": 0.0, "valid1_recurrence_beam_acc_n_predictions_8": 0.0, "valid1_recurrence_beam_acc_n_predictions_9": 0.0, "valid1_recurrence_beam_acc_n_predictions_10": 0.0, "valid1_recurrence_beam_acc_n_input_points_5": 0.0, "valid1_recurrence_beam_acc_n_input_points_6": 0.0, "valid1_recurrence_beam_acc_n_input_points_7": 0.0, "valid1_recurrence_beam_acc_n_input_points_8": 0.0, "valid1_recurrence_beam_acc_n_input_points_9": 0.0, "valid1_recurrence_beam_acc_n_input_points_10": 0.0, "valid1_recurrence_beam_acc_n_input_points_11": 0.0, "valid1_recurrence_beam_acc_n_input_points_12": 0.0, "valid1_recurrence_beam_acc_n_input_points_13": 0.0, "valid1_recurrence_beam_acc_n_input_points_14": 0.0, "valid1_recurrence_beam_acc_n_input_points_15": 0.0, "valid1_recurrence_beam_acc_n_input_points_16": 0.0, "valid1_recurrence_beam_acc_n_input_points_17": 0.0, "valid1_recurrence_beam_acc_n_input_points_18": 0.0, "valid1_recurrence_beam_acc_n_input_points_19": 0.0, "valid1_recurrence_beam_acc_n_input_points_20": 0.0, "valid1_recurrence_beam_acc_n_input_points_21": 0.0, "valid1_recurrence_beam_acc_n_input_points_22": 0.0, "valid1_recurrence_beam_acc_n_input_points_23": 0.0, "valid1_recurrence_beam_acc_n_input_points_24": 0.0, "valid1_recurrence_beam_acc_n_input_points_25": 0.0, "valid1_recurrence_beam_acc_n_input_points_26": 0.0, "valid1_recurrence_beam_acc_n_input_points_27": 0.0, "valid1_recurrence_beam_acc_n_input_points_28": 0.0, "valid1_recurrence_beam_acc_n_input_points_29": 0.0, "valid1_recurrence_beam_acc_n_input_points_30": 0.0, "valid1_recurrence_beam_acc_n_ops_1": 0.0, "valid1_recurrence_beam_acc_n_ops_2": 0.0, "valid1_recurrence_beam_acc_n_ops_3": 0.0, "valid1_recurrence_beam_acc_n_ops_4": 0.0, "valid1_recurrence_beam_acc_n_ops_5": 0.0, "valid1_recurrence_beam_acc_n_ops_6": 0.0, "valid1_recurrence_beam_acc_n_ops_7": 0.0, "valid1_recurrence_beam_acc_n_ops_8": 0.0, "valid1_recurrence_beam_acc_n_ops_9": 0.0, "valid1_recurrence_beam_acc_n_ops_10": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_0": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_1": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_2": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_3": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_4": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_5": 0.0, "valid1_recurrence_beam_acc_n_recurrence_degree_6": 0.0}
INFO - 07/18/22 18:04:01 - 0:00:27 - ============ Starting epoch 0 ... ============
INFO - 07/18/22 18:04:02 - 0:00:28 -      10 -   11.31 equations/s -   349.76 words/s -  MEM: 0.00 MB - RECURRENCE:  6.8948 - LR: 2.9990e-07
INFO - 07/18/22 18:04:06 - 0:00:32 -      20 -   86.33 equations/s -  5607.50 words/s -  MEM: 0.00 MB - RECURRENCE:  6.9069 - LR: 4.9980e-07
INFO - 07/18/22 18:04:09 - 0:00:35 -      30 -  113.69 equations/s -  6614.65 words/s -  MEM: 0.00 MB - RECURRENCE:  6.8855 - LR: 6.9970e-07
INFO - 07/18/22 18:04:12 - 0:00:38 -      40 -   92.80 equations/s -  6083.83 words/s -  MEM: 0.00 MB - RECURRENCE:  6.8869 - LR: 8.9960e-07
INFO - 07/18/22 18:04:15 - 0:00:41 -      50 -  135.17 equations/s -  6699.92 words/s -  MEM: 0.00 MB - RECURRENCE:  6.8187 - LR: 1.0995e-06
INFO - 07/18/22 18:04:17 - 0:00:43 -      60 -  117.78 equations/s -  6040.11 words/s -  MEM: 0.00 MB - RECURRENCE:  6.8463 - LR: 1.2994e-06
INFO - 07/18/22 18:04:20 - 0:00:46 -      70 -  105.56 equations/s -  5510.66 words/s -  MEM: 0.00 MB - RECURRENCE:  6.8041 - LR: 1.4993e-06
INFO - 07/18/22 18:04:24 - 0:00:50 -      80 -   99.04 equations/s -  5577.24 words/s -  MEM: 0.00 MB - RECURRENCE:  6.7732 - LR: 1.6992e-06
INFO - 07/18/22 18:04:27 - 0:00:53 -      90 -   95.08 equations/s -  6104.07 words/s -  MEM: 0.00 MB - RECURRENCE:  6.7212 - LR: 1.8991e-06
INFO - 07/18/22 18:04:29 - 0:00:55 -     100 -  133.02 equations/s -  6333.31 words/s -  MEM: 0.00 MB - RECURRENCE:  6.7851 - LR: 2.0990e-06
